{"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":64900,"databundleVersionId":7253137,"sourceType":"competition"}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Sentence Transformers for Question Answering\n\nThis Jupyter notebook demonstrates the use of Sentence Transformers for building a question-answering model.\n\nIt includes installation of necessary libraries, data preprocessing, model training, evaluation, and submission to the Kaggle competition.\n\n> Thanks: https://www.kaggle.com/code/dtakeshi/dtc-zoomcamp-q-a-sentence-transformers\n\n## Setup\n\n- This section covers the installation of the sentence-transformers and datasets libraries, and imports necessary Python modules.","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5"}},{"cell_type":"code","source":"# Attention we worked on Kaggle GPU-P100\n!pip install -Uq sentence-transformers datasets nltk session_info pipenv","metadata":{"execution":{"iopub.status.busy":"2024-01-26T05:12:28.898139Z","iopub.execute_input":"2024-01-26T05:12:28.898531Z","iopub.status.idle":"2024-01-26T05:12:55.550224Z","shell.execute_reply.started":"2024-01-26T05:12:28.898497Z","shell.execute_reply":"2024-01-26T05:12:55.548938Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ncuml 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ndask-cudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ncudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.0.3 which is incompatible.\ncudf 23.8.0 requires protobuf<5,>=4.21, but you have protobuf 3.20.3 which is incompatible.\ncuml 23.8.0 requires dask==2023.7.1, but you have dask 2023.12.1 which is incompatible.\ncuml 23.8.0 requires distributed==2023.7.1, but you have distributed 2023.12.1 which is incompatible.\ndask-cuda 23.8.0 requires dask==2023.7.1, but you have dask 2023.12.1 which is incompatible.\ndask-cuda 23.8.0 requires distributed==2023.7.1, but you have distributed 2023.12.1 which is incompatible.\ndask-cuda 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.0.3 which is incompatible.\ndask-cudf 23.8.0 requires dask==2023.7.1, but you have dask 2023.12.1 which is incompatible.\ndask-cudf 23.8.0 requires distributed==2023.7.1, but you have distributed 2023.12.1 which is incompatible.\ndask-cudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.0.3 which is incompatible.\ngcsfs 2023.6.0 requires fsspec==2023.6.0, but you have fsspec 2023.10.0 which is incompatible.\npreprocessing 0.1.13 requires nltk==3.2.4, but you have nltk 3.8.1 which is incompatible.\nraft-dask 23.8.0 requires dask==2023.7.1, but you have dask 2023.12.1 which is incompatible.\nraft-dask 23.8.0 requires distributed==2023.7.1, but you have distributed 2023.12.1 which is incompatible.\ns3fs 2023.12.2 requires fsspec==2023.12.2, but you have fsspec 2023.10.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport re\nimport nltk\nimport string\nimport numpy as np\nimport pandas as pd\n\nimport torch\nfrom sentence_transformers import SentenceTransformer, models, evaluation\nfrom sentence_transformers import losses, InputExample  # SentencesDataset\nfrom torch.utils.data import DataLoader\nfrom sentence_transformers import util\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.preprocessing import StandardScaler\n\nfrom datasets import load_dataset\nimport datasets; datasets.disable_progress_bar()\n\nfrom tqdm.notebook import tqdm\ntqdm._instances.clear()\n# Use tqdm.pandas() to enable with progress_apply\ntqdm.pandas()\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport gc\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2024-01-26T05:12:55.552529Z","iopub.execute_input":"2024-01-26T05:12:55.552888Z","iopub.status.idle":"2024-01-26T05:13:02.219728Z","shell.execute_reply.started":"2024-01-26T05:12:55.552855Z","shell.execute_reply":"2024-01-26T05:13:02.218779Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"180"},"metadata":{}}]},{"cell_type":"code","source":"import nltk\nfrom nltk.tokenize import sent_tokenize","metadata":{"execution":{"iopub.status.busy":"2024-01-26T05:13:02.221102Z","iopub.execute_input":"2024-01-26T05:13:02.221799Z","iopub.status.idle":"2024-01-26T05:13:02.226835Z","shell.execute_reply.started":"2024-01-26T05:13:02.221735Z","shell.execute_reply":"2024-01-26T05:13:02.225814Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# !pip install session_info -Uq\nimport session_info\nsession_info.show(html=False)","metadata":{"execution":{"iopub.status.busy":"2024-01-26T05:13:02.229971Z","iopub.execute_input":"2024-01-26T05:13:02.230564Z","iopub.status.idle":"2024-01-26T05:13:02.885129Z","shell.execute_reply.started":"2024-01-26T05:13:02.230534Z","shell.execute_reply":"2024-01-26T05:13:02.884128Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"-----\ndatasets                    2.16.1\nnltk                        3.2.4\nnumpy                       1.24.3\npandas                      2.1.4\nsentence_transformers       2.2.2\nsession_info                1.0.0\nsklearn                     1.2.2\ntorch                       2.0.0\ntqdm                        4.66.1\n-----\nIPython             8.14.0\njupyter_client      7.4.9\njupyter_core        5.3.1\njupyterlab          4.0.10\nnotebook            6.5.6\n-----\nPython 3.10.12 | packaged by conda-forge | (main, Jun 23 2023, 22:40:32) [GCC 12.3.0]\nLinux-5.15.133+-x86_64-with-glibc2.35\n-----\nSession information updated at 2024-01-26 05:13\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Data Loading\n\nLoads training and test data from CSV files.","metadata":{}},{"cell_type":"code","source":"train_questions_df = pd.read_csv(\"/kaggle/input/dtc-zoomcamp-qa-challenge/train_questions.csv\")\ntrain_answers_df   = pd.read_csv(\"/kaggle/input/dtc-zoomcamp-qa-challenge/train_answers.csv\")\ntest_questions_df  = pd.read_csv(\"/kaggle/input/dtc-zoomcamp-qa-challenge/test_questions.csv\")\ntest_answers_df    = pd.read_csv(\"/kaggle/input/dtc-zoomcamp-qa-challenge/test_answers.csv\")\n\ntrain_questions_df.shape, train_answers_df.shape, test_questions_df.shape, test_answers_df.shape,","metadata":{"execution":{"iopub.status.busy":"2024-01-26T05:13:02.886981Z","iopub.execute_input":"2024-01-26T05:13:02.887658Z","iopub.status.idle":"2024-01-26T05:13:02.969476Z","shell.execute_reply.started":"2024-01-26T05:13:02.887618Z","shell.execute_reply":"2024-01-26T05:13:02.968570Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"((397, 6), (397, 5), (516, 5), (516, 5))"},"metadata":{}}]},{"cell_type":"code","source":"# Check Duplicates\nprint(train_questions_df.duplicated('question_id').sum())\nprint(train_answers_df.duplicated('answer_id').sum())\nprint(test_questions_df.duplicated('question_id').sum())\nprint(test_answers_df.duplicated('answer_id').sum())","metadata":{"execution":{"iopub.status.busy":"2024-01-26T05:13:02.970829Z","iopub.execute_input":"2024-01-26T05:13:02.971173Z","iopub.status.idle":"2024-01-26T05:13:02.986846Z","shell.execute_reply.started":"2024-01-26T05:13:02.971145Z","shell.execute_reply":"2024-01-26T05:13:02.985732Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"1\n1\n2\n1\n","output_type":"stream"}]},{"cell_type":"code","source":"# Remove Duplicates\ntrain_questions_df = train_questions_df.drop_duplicates('question_id').copy()\ntrain_answers_df   = train_answers_df.drop_duplicates('answer_id').copy()\ntest_questions_df  = test_questions_df.drop_duplicates('question_id').copy()\ntest_answers_df    = test_answers_df.drop_duplicates('answer_id').copy()","metadata":{"execution":{"iopub.status.busy":"2024-01-26T05:13:02.988214Z","iopub.execute_input":"2024-01-26T05:13:02.988931Z","iopub.status.idle":"2024-01-26T05:13:03.003367Z","shell.execute_reply.started":"2024-01-26T05:13:02.988893Z","shell.execute_reply":"2024-01-26T05:13:03.002172Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"## Data Wrangling\n\n- Merging several data sources into one data-set for analysis.\n- Identifying gaps or empty cells in data and either filling or removing them.\n- Deleting irrelevant or unnecessary data.\n- Identifying severe outliers in data and either explaining the inconsistencies or deleting them to facilitate analysis.\n\n### Data Preprocessing¶\n\n- Processes training data to create triplets of question, positive answer, and negative answers.","metadata":{}},{"cell_type":"code","source":"train_questions_df_triplets = pd.DataFrame()\n\nfor question_id, question, course, year, candidate_answers, answer_id in train_questions_df.values:\n    answers_list = list(map(int, candidate_answers.split(',')))\n    negative_ids = [x for x in answers_list if x != answer_id]\n    negative_list, positive_list = [],[]    \n        \n    positive_list.append(train_answers_df[train_answers_df.answer_id == int(answer_id)]['answer'].values[0])\n    for neg_id in negative_ids:\n        negative_list.append(train_answers_df[train_answers_df.answer_id == int(neg_id)]['answer'].values[0])\n    \n    # Adding a single new row\n    new_row = {\n        'question_id'  : question_id, \n        'question'     : question, \n        'positive'     : positive_list[0],\n        'negatives'    : negative_list,\n        #'answer_id'    : answer_id,\n        #'relevant_docs': answers_list,\n    }\n    \n    train_questions_df_triplets = train_questions_df_triplets._append(new_row, ignore_index=True)","metadata":{"execution":{"iopub.status.busy":"2024-01-26T05:13:03.004673Z","iopub.execute_input":"2024-01-26T05:13:03.005046Z","iopub.status.idle":"2024-01-26T05:13:04.302803Z","shell.execute_reply.started":"2024-01-26T05:13:03.005009Z","shell.execute_reply":"2024-01-26T05:13:04.301730Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"train_questions_df_triplets","metadata":{"execution":{"iopub.status.busy":"2024-01-26T05:13:04.304327Z","iopub.execute_input":"2024-01-26T05:13:04.304738Z","iopub.status.idle":"2024-01-26T05:13:04.328182Z","shell.execute_reply.started":"2024-01-26T05:13:04.304697Z","shell.execute_reply":"2024-01-26T05:13:04.327257Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"     question_id                                           question  \\\n0          79062  For categorical target set, where the distribu...   \n1         468946  Is there anything that we are not allowed to u...   \n2         968800  I have been catching up and have been doing ho...   \n3         688404  Could you please explain what code we should l...   \n4          63921  Is it just me or does the model have really ba...   \n..           ...                                                ...   \n391       241788  Can the model with the ROC AUC score of around...   \n392       595103  When I click tab in the parentheses of the iPy...   \n393       450348  Can you please explain the use cases of Splunk...   \n394       864660  Why did you use model2bin in the last question...   \n395       205640  Would you mind clarifying evaluation on test a...   \n\n                                              positive  \\\n0    Alexey\\nShould we use something non-standard t...   \n1    No, I don't think there is anything you cannot...   \n2    Alexey\\nYes, you will be. You can submit the p...   \n3    Alexey\\nI think the question refers to the hom...   \n4    Dmitry\\nIt's fine, because this is the showcas...   \n..                                                 ...   \n391  Yes, it can. It's really dataset dependent. Fo...   \n392  Let's say I do “import numpy as np” and then, ...   \n393  Alexey\\nSplunk – I don’t know. It's not a data...   \n394  Yes, it was not mentioned. But what was mentio...   \n395  Alexey\\nI guess the question is why we need to...   \n\n                                             negatives  \n0    [I don't know if you're referring to this. [im...  \n1    [44. Two people submitted twice. So actually, ...  \n2    [Ankush\\nI don't know if there's an example pr...  \n3    [Alexey\\nIt's not over for you, because we onl...  \n4    [I don't know if you're referring to this. [im...  \n..                                                 ...  \n391  [I took this course a while ago. Back then it ...  \n392  [You mean another iteration of the course? Yes...  \n393  [Ankush\\nIf you're talking specifically about ...  \n394  [If two people want to work on the same datase...  \n395  [No, you do not. Building locally is fine., I ...  \n\n[396 rows x 4 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>question_id</th>\n      <th>question</th>\n      <th>positive</th>\n      <th>negatives</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>79062</td>\n      <td>For categorical target set, where the distribu...</td>\n      <td>Alexey\\nShould we use something non-standard t...</td>\n      <td>[I don't know if you're referring to this. [im...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>468946</td>\n      <td>Is there anything that we are not allowed to u...</td>\n      <td>No, I don't think there is anything you cannot...</td>\n      <td>[44. Two people submitted twice. So actually, ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>968800</td>\n      <td>I have been catching up and have been doing ho...</td>\n      <td>Alexey\\nYes, you will be. You can submit the p...</td>\n      <td>[Ankush\\nI don't know if there's an example pr...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>688404</td>\n      <td>Could you please explain what code we should l...</td>\n      <td>Alexey\\nI think the question refers to the hom...</td>\n      <td>[Alexey\\nIt's not over for you, because we onl...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>63921</td>\n      <td>Is it just me or does the model have really ba...</td>\n      <td>Dmitry\\nIt's fine, because this is the showcas...</td>\n      <td>[I don't know if you're referring to this. [im...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>391</th>\n      <td>241788</td>\n      <td>Can the model with the ROC AUC score of around...</td>\n      <td>Yes, it can. It's really dataset dependent. Fo...</td>\n      <td>[I took this course a while ago. Back then it ...</td>\n    </tr>\n    <tr>\n      <th>392</th>\n      <td>595103</td>\n      <td>When I click tab in the parentheses of the iPy...</td>\n      <td>Let's say I do “import numpy as np” and then, ...</td>\n      <td>[You mean another iteration of the course? Yes...</td>\n    </tr>\n    <tr>\n      <th>393</th>\n      <td>450348</td>\n      <td>Can you please explain the use cases of Splunk...</td>\n      <td>Alexey\\nSplunk – I don’t know. It's not a data...</td>\n      <td>[Ankush\\nIf you're talking specifically about ...</td>\n    </tr>\n    <tr>\n      <th>394</th>\n      <td>864660</td>\n      <td>Why did you use model2bin in the last question...</td>\n      <td>Yes, it was not mentioned. But what was mentio...</td>\n      <td>[If two people want to work on the same datase...</td>\n    </tr>\n    <tr>\n      <th>395</th>\n      <td>205640</td>\n      <td>Would you mind clarifying evaluation on test a...</td>\n      <td>Alexey\\nI guess the question is why we need to...</td>\n      <td>[No, you do not. Building locally is fine., I ...</td>\n    </tr>\n  </tbody>\n</table>\n<p>396 rows × 4 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ntrain_df, val_df = train_test_split(train_questions_df_triplets, test_size=0.2, random_state=42)\n\n# Display the shapes of the resulting sets\nprint(\"Train set shape     :\", train_df.shape)\nprint(\"Validation set shape:\", val_df.shape)","metadata":{"execution":{"iopub.status.busy":"2024-01-26T05:13:04.331758Z","iopub.execute_input":"2024-01-26T05:13:04.332101Z","iopub.status.idle":"2024-01-26T05:13:04.340561Z","shell.execute_reply.started":"2024-01-26T05:13:04.332071Z","shell.execute_reply":"2024-01-26T05:13:04.339560Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Train set shape     : (316, 4)\nValidation set shape: (80, 4)\n","output_type":"stream"}]},{"cell_type":"code","source":"df_train = train_questions_df_triplets.iloc[train_df.index]\ndf_train.shape","metadata":{"execution":{"iopub.status.busy":"2024-01-26T05:13:04.342194Z","iopub.execute_input":"2024-01-26T05:13:04.342936Z","iopub.status.idle":"2024-01-26T05:13:04.353595Z","shell.execute_reply.started":"2024-01-26T05:13:04.342880Z","shell.execute_reply":"2024-01-26T05:13:04.352667Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"(316, 4)"},"metadata":{}}]},{"cell_type":"markdown","source":"## Model Initialization\n\nInitializes the Sentence Transformer model.\n\n```\nSiamese Neural Networks Architecture:\n\nInput 1  |      |-> Subnetwork 1 ---|\n         |      |                   |      Triplet loss                                         Manhattan-L1 distance\nInput 2  |----->|-> Subnetwork 2 ---|--> ( Contrastive loss )--> Output (Distance/Similarity) ( Euclidean distance    )\n         |      |                   |      Binary loss                                          Angular Cosine distance\nLabels   |      |_ _ _ _ _ _ _ _ _ _|\n```\n\nPretrained Models\n* https://www.sbert.net/docs/pretrained_models.html\n* https://github.com/microsoft/unilm/tree/master/minilm\n    * https://huggingface.co/microsoft/MiniLM-L12-H384-uncased\n\n\nUsing the Model 'all-mpnet-base-v2' from HuggingFace\n\nThe all-* models where trained on all available training data (more than 1 billion training pairs) and are designed as general purpose models. \n\nThe all-mpnet-base-v2 model provides the best quality, while all-MiniLM-L6-v2 is 5 times faster and still offers good quality.","metadata":{"execution":{"iopub.execute_input":"2024-01-08T21:23:46.229913Z","iopub.status.busy":"2024-01-08T21:23:46.229414Z","iopub.status.idle":"2024-01-08T21:23:46.235795Z","shell.execute_reply":"2024-01-08T21:23:46.234349Z","shell.execute_reply.started":"2024-01-08T21:23:46.229874Z"}}},{"cell_type":"code","source":"from transformers import AutoConfig, AutoTokenizer, AutoModel, TFAutoModel\n# from transformers import AutoModelForSequenceClassification, TFAutoModelForSequenceClassification\n\n## By default, input text longer than 384 word pieces is truncated.\n## It maps sentences & paragraphs to a 768 dimensional dense vector space.\n## fine-tuned in on a 1B sentence pairs dataset.\nmodel_checkpoint = 'sentence-transformers/all-mpnet-base-v2'\n\n## By default, input text longer than 512 word pieces is truncated.\n## It maps sentences & paragraphs to a 768 dimensional dense vector space.\n## trained on 215M (question, answer) pairs from diverse sources.\n# model_checkpoint = 'sentence-transformers/multi-qa-mpnet-base-dot-v1'\n# model_checkpoint = 'sentence-transformers/multi-qa-mpnet-base-cos-v1'\n\n# Load the tokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\ntokenizer.max_len_single_sentence","metadata":{"execution":{"iopub.status.busy":"2024-01-26T05:14:46.750345Z","iopub.execute_input":"2024-01-26T05:14:46.750779Z","iopub.status.idle":"2024-01-26T05:14:46.889777Z","shell.execute_reply.started":"2024-01-26T05:14:46.750729Z","shell.execute_reply":"2024-01-26T05:14:46.888809Z"},"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"510"},"metadata":{}}]},{"cell_type":"code","source":"from sentence_transformers import SentenceTransformer, models, evaluation\n\n# Load the model\nmodel = SentenceTransformer(model_checkpoint)\nmodel.max_seq_length = 512\nmodel = model.to('cuda')\nmodel","metadata":{"execution":{"iopub.status.busy":"2024-01-26T05:15:02.933122Z","iopub.execute_input":"2024-01-26T05:15:02.933513Z","iopub.status.idle":"2024-01-26T05:15:03.466882Z","shell.execute_reply.started":"2024-01-26T05:15:02.933475Z","shell.execute_reply":"2024-01-26T05:15:03.465922Z"},"trusted":true},"execution_count":18,"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"SentenceTransformer(\n  (0): Transformer({'max_seq_length': 512, 'do_lower_case': False}) with Transformer model: MPNetModel \n  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n  (2): Normalize()\n)"},"metadata":{}}]},{"cell_type":"code","source":"## Step 1: Define two transformer models, max_seq_length=384\nword_embedding_model = models.Transformer(model_checkpoint, max_seq_length=512)\n## Step 2: Define the pooling strategy over the token embeddings\npooling_model = models.Pooling(\n    word_embedding_model.get_word_embedding_dimension(),\n    pooling_mode_cls_token= True,\n    pooling_mode_mean_tokens= True,\n    pooling_mode_max_tokens = True,\n    pooling_mode_mean_sqrt_len_tokens = True,\n)\n## Step 3: Define the dense layer\ndense_layer = models.Dense(\n    in_features=pooling_model.get_sentence_embedding_dimension(),\n    out_features=768,\n)\n## Step 4: Define the normalize layer if needed\nnormalize_layer = models.Normalize()\n## Step 5: Construct the final model\nmodel = SentenceTransformer(modules=[word_embedding_model, pooling_model, dense_layer, normalize_layer], device='cuda')\n\n# Move the model to GPU\n# model = model.to('cuda')\nmodel","metadata":{"execution":{"iopub.status.busy":"2024-01-26T05:15:17.794456Z","iopub.execute_input":"2024-01-26T05:15:17.795173Z","iopub.status.idle":"2024-01-26T05:15:18.387471Z","shell.execute_reply.started":"2024-01-26T05:15:17.795135Z","shell.execute_reply":"2024-01-26T05:15:18.386495Z"},"trusted":true},"execution_count":20,"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"SentenceTransformer(\n  (0): Transformer({'max_seq_length': 512, 'do_lower_case': False}) with Transformer model: MPNetModel \n  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': True, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': True, 'pooling_mode_mean_sqrt_len_tokens': True})\n  (2): Dense({'in_features': 3072, 'out_features': 768, 'bias': True, 'activation_function': 'torch.nn.modules.activation.Tanh'})\n  (3): Normalize()\n)"},"metadata":{}}]},{"cell_type":"markdown","source":"## Evaluation Formula for both dot product and cosine similarity:\n\n* The dot product (also known as the inner product or scalar product) between two vectors\n* A and B is calculated as the sum of the element-wise products:\n    * > $ \\text{Dot Product}(A, B) = A \\cdot B = \\sum_{i=1}^{n} A_i \\cdot B_i $\n* The cosine similarity between two vectors A and B is calculated as the normalized dot product:\n* where ∥A∥ and ∥B∥ represent the Euclidean norms (magnitudes) of the vectors A and B respectively.\n    * > $ \\text{Cosine Similarity}(A, B) = \\frac{A \\cdot B}{\\|A\\| \\cdot \\|B\\|} $\n    * > $ \\|\\mathbf{A}\\| = \\sqrt{\\sum_{i=1}^{n} A_i^2},  \\|\\mathbf{B}\\| = \\sqrt{\\sum_{i=1}^{n} B_i^2} $","metadata":{}},{"cell_type":"code","source":"from sentence_transformers import util\nfrom sklearn.metrics.pairwise import cosine_similarity\n\nquery_embedding   = model.encode('How big is London', show_progress_bar=0)\npassage_embedding = model.encode([\n    'London has 9,787,426 inhabitants at the 2011 census',\n    'London is known for its finacial district'\n], show_progress_bar=0)\n\nprint(f\"Dot Similarity   : {util.dot_score(query_embedding, passage_embedding).tolist()}\")\nprint(f\"Cosine Similarity: {cosine_similarity(query_embedding.reshape(1, -1), passage_embedding)}\")","metadata":{"execution":{"iopub.status.busy":"2024-01-26T05:15:25.877551Z","iopub.execute_input":"2024-01-26T05:15:25.878193Z","iopub.status.idle":"2024-01-26T05:15:26.891145Z","shell.execute_reply.started":"2024-01-26T05:15:25.878159Z","shell.execute_reply":"2024-01-26T05:15:26.890156Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"Dot Similarity   : [[0.6112002730369568, 0.48592284321784973]]\nCosine Similarity: [[0.61120033 0.48592287]]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Post-processing, check validation initial scores","metadata":{}},{"cell_type":"code","source":"from nltk.tokenize import sent_tokenize\n\n# SEED = 42\n# np.random.seed(SEED)\n# Function to split, filter and shuffle into sentence pieces\ndef process_sentences(text, min_sent=7, shuffle=False):\n    \"\"\"Split long sentences small part\"\"\"\n    sentences = sent_tokenize(text)\n    sentences = np.split(sentences, range(min_sent, len(sentences), min_sent))\n    sentences = filter(lambda x: len(x), sentences)\n    sentences = [np.random.permutation(i) if shuffle else i for i in sentences]\n    sentences = list(map(' '.join, sentences))\n    return sentences","metadata":{"execution":{"iopub.status.busy":"2024-01-26T05:15:46.639196Z","iopub.execute_input":"2024-01-26T05:15:46.639573Z","iopub.status.idle":"2024-01-26T05:15:46.646802Z","shell.execute_reply.started":"2024-01-26T05:15:46.639542Z","shell.execute_reply":"2024-01-26T05:15:46.645621Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"try:\n    del train_questions_df['predictions']\nexcept:\n    pass\n\ntrain_questions_df_predict = pd.DataFrame()\n\nfor question_id, question, course, year, candidate_answers, answer_id in tqdm(train_questions_df.values):\n    answers_list = candidate_answers.split(',')\n    \n    candidate_answers_list = []\n    for ans_id in answers_list:\n        candidate_answers_list.append(train_answers_df[train_answers_df.answer_id == int(ans_id)]['answer'].values[0])\n\n    # Adding a single new row\n    new_row = {\n        'question_id': question_id, \n        'question': question, \n        'candidate_answers_id': answers_list,\n        'candidate_answers': candidate_answers_list,\n    }\n\n    train_questions_df_predict = train_questions_df_predict._append(new_row, ignore_index=True)\n    \n    \ntrain_predictions = []\nsimilarity_scores_df = []\nfor question, candidate_answers, candidate_answers_id in zip(\n    tqdm(train_questions_df_predict['question']), \n    train_questions_df_predict['candidate_answers'],\n    train_questions_df_predict['candidate_answers_id']\n):\n    q_embeddings = model.encode([question], show_progress_bar=0)\n    a_embeddings = []    \n    for answer in candidate_answers:\n        a_embedding = np.mean(model.encode(process_sentences(answer), show_progress_bar=0), axis=0)\n        a_embeddings.append(a_embedding)    \n    similarity_scores = cosine_similarity(q_embeddings, a_embeddings)\n    \n    train_predictions.append(int(candidate_answers_id[np.argmax(similarity_scores)]))\n    similarity_scores_df.append(similarity_scores)\n\ntrain_questions_df['predictions'] = train_predictions","metadata":{"execution":{"iopub.status.busy":"2024-01-26T05:15:50.801614Z","iopub.execute_input":"2024-01-26T05:15:50.801995Z","iopub.status.idle":"2024-01-26T05:16:46.439386Z","shell.execute_reply.started":"2024-01-26T05:15:50.801963Z","shell.execute_reply":"2024-01-26T05:16:46.438344Z"},"trusted":true},"execution_count":23,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/396 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"73a12ea2794042e386a982fe6766c734"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/396 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1a88ab0a12a2429ca0f675915d0807f4"}},"metadata":{}}]},{"cell_type":"code","source":"# Accuracy calculation\naccuracy_full  = (train_questions_df['predictions'] == train_questions_df['answer_id']).mean()\nprint(f'Accuracy full : {accuracy_full:.4f}')\n\naccuracy_val   = train_questions_df[train_questions_df['question_id'].isin(val_df['question_id']  )].apply(lambda row: row['predictions']==row['answer_id'], axis=1).mean()\naccuracy_train = train_questions_df[train_questions_df['question_id'].isin(train_df['question_id'])].apply(lambda row: row['predictions']==row['answer_id'], axis=1).mean()\nprint(f'Accuracy val  : {accuracy_val:.4f}')\nprint(f'Accuracy train: {accuracy_train:.4f}')","metadata":{"execution":{"iopub.status.busy":"2024-01-26T05:16:47.615080Z","iopub.execute_input":"2024-01-26T05:16:47.615927Z","iopub.status.idle":"2024-01-26T05:16:47.636133Z","shell.execute_reply.started":"2024-01-26T05:16:47.615888Z","shell.execute_reply":"2024-01-26T05:16:47.635095Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"Accuracy full : 0.9242\nAccuracy val  : 0.9250\nAccuracy train: 0.9241\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## evaluation dataset","metadata":{}},{"cell_type":"code","source":"from sentence_transformers import evaluation\n\nBATCH_SIZE = 16\nevaluators = []\n\n# Create lists to store the transformed evaluation data\nsentences1, sentences2, labels = [], [], []\nfor idx, row in tqdm(val_df.iterrows(), total=len(val_df)):\n    question = row['question']\n    positive = row['positive']\n    negative_list = row.get('negatives', [])  # Handle the case where 'negatives' is missing\n\n    # Append the data for positive example\n    sentences1.append(question)\n    sentences2.append(positive)\n    labels.append(1)  # Assuming positive examples should be labeled as 1\n\n    # Append the data for each negative example\n    for negative in negative_list:\n        sentences1.append(question)\n        sentences2.append(negative)\n        labels.append(0)  # Assuming negative examples should be labeled as 0\n        \nprint(\"Number of Binary Evaluator Samples:\", len(labels))    \n# Initialize the BinaryClassificationEvaluator with sentences1, sentences2, and labels\nbinary_acc_evaluator = evaluation.BinaryClassificationEvaluator(sentences1, sentences2, labels, batch_size=BATCH_SIZE)\nevaluators.append(binary_acc_evaluator)\n\n# Evaluate the model and get the results\n# results = binary_evaluator.compute_metrices(model)['dot'][\"ap\"]\nresults = binary_acc_evaluator(model)\nprint(f\"Binary Accuracy Evaluator Results : {results:.4f}\")","metadata":{"execution":{"iopub.status.busy":"2024-01-26T05:16:51.477882Z","iopub.execute_input":"2024-01-26T05:16:51.478265Z","iopub.status.idle":"2024-01-26T05:16:53.185742Z","shell.execute_reply.started":"2024-01-26T05:16:51.478236Z","shell.execute_reply":"2024-01-26T05:16:53.184807Z"},"trusted":true},"execution_count":25,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/80 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3c515fecf58f40ee8c8cdd3f3a7348a7"}},"metadata":{}},{"name":"stdout","text":"Number of Binary Evaluator Samples: 400\nBinary Accuracy Evaluator Results : 0.9140\n","output_type":"stream"}]},{"cell_type":"code","source":"# Create lists to store the transformed evaluation data\nanchors, positives, negatives = [], [], []\nfor idx, row in tqdm(val_df.iterrows(), total=len(val_df)):\n    question = row['question']\n    positive = row['positive']\n    negative_list = row.get('negatives', [])  # Handle the case where 'negatives' is missing\n\n    # Append the data for each question, positive, negative example\n    for negative in negative_list:\n        anchors.append(question)\n        positives.append(positive)\n        negatives.append(negative)\n\nprint(\"Number of Triplet Evaluator Samples:\", len(anchors))\n# Initialize the TripletEvaluator with sentences1, sentences2, and labels\ntriplet_evaluator = evaluation.TripletEvaluator(anchors, positives, negatives, batch_size=BATCH_SIZE)\nevaluators.append(triplet_evaluator)\n\n# Evaluate the model and get the results\nresults = triplet_evaluator(model)\nprint(f\"Triplet Evaluator Results          : {results:.4f}\")","metadata":{"execution":{"iopub.status.busy":"2024-01-26T05:16:55.264712Z","iopub.execute_input":"2024-01-26T05:16:55.265105Z","iopub.status.idle":"2024-01-26T05:17:01.923989Z","shell.execute_reply.started":"2024-01-26T05:16:55.265076Z","shell.execute_reply":"2024-01-26T05:17:01.922902Z"},"trusted":true},"execution_count":26,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/80 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"62a478885ce24f3cad16d594672d6c3c"}},"metadata":{}},{"name":"stdout","text":"Number of Triplet Evaluator Samples: 320\nTriplet Evaluator Results          : 0.9688\n","output_type":"stream"}]},{"cell_type":"code","source":"# Create lists to store the transformed evaluation data\nir_queries       = {}     # qid => query (qid => question)\nir_corpus        = {}     # cid => doc (qid => positive)\nir_relevant_docs = {}     # qid => Set[cid] Mapping of relevant documents for a given query (qid => set([relevant_answer_ids])\n# ir_needed_qids   = set()  # QIDs we need in the corpus\n\nfor idx, row in tqdm(val_df.iterrows(), total=len(val_df)):\n    question = row['question']\n    positive = row['positive']\n    qid      = row['question_id']\n    relevant_docs = set([qid])\n\n    # Populate the dictionaries\n    ir_queries[qid] = question\n    ir_corpus[qid]  = positive  \n    ir_relevant_docs[qid] = relevant_docs\n    # ir_needed_qids.add(qid)\n\n# Create an instance of InformationRetrievalEvaluator\nir_evaluator = evaluation.InformationRetrievalEvaluator(ir_queries, ir_corpus, ir_relevant_docs)\nevaluators.append(ir_evaluator)\n\n# Evaluate the model and get the results\nresults = ir_evaluator(model)\nprint(f\"InformationRetrieval Evaluator Results: {results:.4f}\")","metadata":{"execution":{"iopub.status.busy":"2024-01-26T05:17:01.925912Z","iopub.execute_input":"2024-01-26T05:17:01.926233Z","iopub.status.idle":"2024-01-26T05:17:03.510530Z","shell.execute_reply.started":"2024-01-26T05:17:01.926205Z","shell.execute_reply":"2024-01-26T05:17:03.509546Z"},"trusted":true},"execution_count":27,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/80 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5bd669c0a375445282ca40fb8c6e4ee8"}},"metadata":{}},{"name":"stdout","text":"InformationRetrieval Evaluator Results: 0.8509\n","output_type":"stream"}]},{"cell_type":"code","source":"# Create a SequentialEvaluator. This SequentialEvaluator runs all three evaluators in a sequential order.\n# We optimize the model with respect to the score from the last evaluator (scores[0])\nseq_evaluator = evaluation.SequentialEvaluator(evaluators, main_score_function=lambda scores: scores[:])\nseq_evaluator(model, epoch=0, steps=0)","metadata":{"execution":{"iopub.status.busy":"2024-01-26T05:17:03.511913Z","iopub.execute_input":"2024-01-26T05:17:03.512237Z","iopub.status.idle":"2024-01-26T05:17:12.912709Z","shell.execute_reply.started":"2024-01-26T05:17:03.512208Z","shell.execute_reply":"2024-01-26T05:17:12.911167Z"},"trusted":true},"execution_count":28,"outputs":[{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"[0.9139851527653133, 0.96875, 0.8509006211180123]"},"metadata":{}}]},{"cell_type":"markdown","source":"## Training Data Preparation\n\nPrepares training examples and dataloader.\n\nThe MultipleNegativesRankingLoss needs a pair of related sentences, in this case, question and answer.\n\nWe could use the TripletLoss loss function with the question as an Anchor, the selected answer as a positive and the other candidate answers as negatives\n\n- https://huggingface.co/blog/how-to-train-sentence-transformers\n\nBelow is a brief explanation of the suitable triplet inputs for each mentioned loss function:\n\n* **MultipleNegativesRankingLoss**: Requires pairs of anchor and positive, and a list of negatives.\n* **CachedMultipleNegativesRankingLoss**: Expects triplets with precomputed negatives for each anchor.\n* **MultipleNegativesSymmetricRankingLoss**: Expects symmetric triplets, i.e., anchor, positive, negative, and vice versa.\n* **TripletLoss**: Standard triplet loss, requires triplets with anchor, positive, and negative.\n* **BatchAllTripletLoss**: This loss considers all possible triplets within a batch.\n* **BatchHardSoftMarginTripletLoss**: Expects triplets where the negative is the hardest negative for the anchor, and the positive is the hardest positive for the anchor.\n* **BatchHardTripletLoss**: Requires triplets where the negative is the hardest negative for the anchor.\n* **BatchSemiHardTripletLoss**: Similar to BatchHardTripletLoss but allows for a marginally softer negative.\n* **MegaBatchMarginLoss**: Expects mega-batch format for triplets.\n* **CosineSimilarityLoss**: Requires pairs of examples, often used in metric learning with cosine similarity.\n* **SoftmaxLoss**: Requires class labels for each example, used in classification tasks.\n* **ContrastiveLoss**: Requires pairs of similar (positive) and dissimilar (negative) examples.\n* **ContrastiveTensionLoss**: Similar to ContrastiveLoss but considers additional tension loss.\n* **OnlineContrastiveLoss**: Similar to ContrastiveLoss but suitable for online learning scenarios.\n* **DenoisingAutoEncoderLoss**: Used for denoising autoencoders, might require corrupted versions of the input.\n* **MSELoss**: Standard mean squared error loss, suitable for regression tasks.\n* **MarginMSELoss**: Similar to MSELoss but with an additional margin.\n\n![](https://i.imgur.com/kV7eWJg.png)","metadata":{}},{"cell_type":"code","source":"from sentence_transformers import losses, InputExample, SentencesDataset\nfrom torch.utils.data import DataLoader\nfrom itertools import cycle, zip_longest\n\ntrain_samples_MultipleNegativesRankingLoss = []\ntrain_samples_ConstrativeLoss = []\nfor question_id, question, positive, negative_list in tqdm(train_df.values):        \n    for negative in negative_list:\n        positive_sents = process_sentences(positive, shuffle=True)\n        negative_sents = process_sentences(negative, shuffle=True)\n        # lower = list(filter(lambda x: len(x) == min(map(len, [positive_sents, negative_sents])), [positive_sents, negative_sents])).pop()\n        # np.random.shuffle(lower)\n        # for positive_sent, negative_sent in zip_longest(positive_sents, negative_sents, fillvalue=lower[0]):\n        for positive_sent, negative_sent in zip(positive_sents, negative_sents):\n            # MultipleNegativesRankingLoss, if A is a positive of B, then B is a positive of A\n            train_samples_MultipleNegativesRankingLoss.append(InputExample(\n                texts=[question, positive_sent, negative_sent], label=1\n            ))\n            train_samples_MultipleNegativesRankingLoss.append(InputExample(\n                texts=[positive_sent, question, negative_sent], label=1\n            ))\n            if np.random.uniform() < 0.7:\n                # ConstrativeLoss\n                train_samples_ConstrativeLoss.append(InputExample(texts=[question, positive_sent], label=1))\n                train_samples_ConstrativeLoss.append(InputExample(texts=[question, negative_sent], label=0))\n            \nprint(f\"Number of MultipleNegativesRankingLoss Trainig Examples: {len(train_samples_MultipleNegativesRankingLoss)}\") \nprint(f\"Number of OnlineContrastiveLoss        Trainig Examples: {len(train_samples_ConstrativeLoss)}\\n\")        \n# Shuffle the training examples\nnp.random.shuffle(train_samples_MultipleNegativesRankingLoss)\nnp.random.shuffle(train_samples_ConstrativeLoss)\n\n# Create data loader and loss for MultipleNegativesRankingLoss\n# train_samples_MultipleNegativesRankingLoss  = SentencesDataset(train_samples_MultipleNegativesRankingLoss, model=model)\ntrain_dataloader_MultipleNegativesRankingLoss = DataLoader(train_samples_MultipleNegativesRankingLoss, shuffle=True, batch_size=BATCH_SIZE)\ntrain_loss_MultipleNegativesRankingLoss       = losses.MultipleNegativesRankingLoss(model)\n\n# Create data loader and loss for OnlineContrastiveLoss, we use cosine distance (cosine_distance = 1-cosine_similarity)\ntrain_dataloader_ConstrativeLoss = DataLoader(train_samples_ConstrativeLoss, shuffle=True, batch_size=BATCH_SIZE)\ntrain_loss_ConstrativeLoss       = losses.OnlineContrastiveLoss(model=model)\n\n## (anchor, positive, negative) negative optional in some, loss function associated with datasets formats.\n# train_loss = losses.ContrastiveLoss(model=model)\n# train_loss = losses.MultipleNegativesSymmetricRankingLoss(model=model)\n# train_loss = losses.MultipleNegativesRankingLoss(model=model)\n# train_loss = losses.TripletLoss(model=model)\nprint(f\"Number of MultipleNegativesRankingLoss Trainig Epoch   : {len(train_dataloader_MultipleNegativesRankingLoss)}\")\nprint(f\"Number of OnlineContrastiveLoss        Trainig Epoch   : {len(train_dataloader_ConstrativeLoss)}\")","metadata":{"execution":{"iopub.status.busy":"2024-01-26T05:17:36.448160Z","iopub.execute_input":"2024-01-26T05:17:36.448559Z","iopub.status.idle":"2024-01-26T05:17:38.743179Z","shell.execute_reply.started":"2024-01-26T05:17:36.448521Z","shell.execute_reply":"2024-01-26T05:17:38.742253Z"},"trusted":true},"execution_count":29,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/316 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"960b7c6fb3f24c708e96326b33468413"}},"metadata":{}},{"name":"stdout","text":"Number of MultipleNegativesRankingLoss Trainig Examples: 4546\nNumber of OnlineContrastiveLoss        Trainig Examples: 3112\n\nNumber of MultipleNegativesRankingLoss Trainig Epoch   : 285\nNumber of OnlineContrastiveLoss        Trainig Epoch   : 195\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Model Training\n\nTrains the model over the specified number of epochs.\n\n- https://github.com/UKPLab/sentence-transformers/tree/master/examples","metadata":{}},{"cell_type":"code","source":"# Configure the training\ninitial_learning_rate = 2e-5  # 2e-5\nweight_decay          = 0.01  # 0.01\nnum_epochs   = 100\nwarmup_steps = np.ceil(len(train_dataloader_MultipleNegativesRankingLoss) * num_epochs * 0.1)  # 10% of train data for warm-up\n\n# Early stopping parameters\nearly_stop = False\nbest_score = float('-inf')\npatience = 2\ncounter  = 0\n\n# Best model file path\nbest_model_path = './best_model'\nbest_model_state_dict = 'best_model.safetensors'\n\n# Train the model\nfor epoch in tqdm(range(num_epochs), desc=\"Training\"):  # Set the number of epochs\n    # cleaning\n    gc.collect()\n    model.fit(\n        train_objectives = [\n            (train_dataloader_MultipleNegativesRankingLoss, train_loss_MultipleNegativesRankingLoss), \n            (train_dataloader_ConstrativeLoss, train_loss_ConstrativeLoss)\n        ],\n        epochs           = 1,\n        optimizer_params = {'lr': initial_learning_rate},\n        weight_decay     = weight_decay,\n        warmup_steps     = warmup_steps,\n        show_progress_bar= 0,                           # Display the progress bar\n        # evaluator      = seq_evaluator,\n        # output_path    = best_model_path,\n        # save_best_model= True\n    )\n    # Evaluate on the validation set\n    score = binary_acc_evaluator(model)\n    print(\n        f\"Epoch {epoch + 1}: \"\n        f\"binary_evaluator Score = {score}, \"\n        f\"triplet_evaluator Score = {triplet_evaluator(model)}, \"\n        f\"ir_evaluator Score = {ir_evaluator(model)}\"\n    )\n    # Check for early stopping\n    if score > best_score:\n        best_score = score\n        counter = 0\n        # Save the trained model\n        torch.save(model.state_dict(), best_model_state_dict)\n        # Use Sentence Transformers' save method\n        # model.save(best_model_path)\n    else:\n        counter += 1\n        # Manually adjust the learning rate after each epoch\n        initial_learning_rate *= 0.85\n        weight_decay          *= 0.85\n        if counter >= patience:\n            print(f\"Early stopping at epoch {epoch + 1 - patience}\")\n            early_stop = True\n            break\n\n# Restore the best weights if early stopping occurred\nif early_stop:\n    # Load the best model state dict into the model\n    model.load_state_dict(torch.load(best_model_state_dict))\n    print(\"Best weights restored.\")","metadata":{"execution":{"iopub.execute_input":"2024-01-25T15:46:18.303334Z","iopub.status.busy":"2024-01-25T15:46:18.302862Z","iopub.status.idle":"2024-01-25T16:37:36.494654Z","shell.execute_reply":"2024-01-25T16:37:36.493494Z","shell.execute_reply.started":"2024-01-25T15:46:18.303299Z"},"trusted":true},"execution_count":30,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"fd558f45cc7d4f7d8bdcede4173e6fc6","version_major":2,"version_minor":0},"text/plain":["Training:   0%|          | 0/100 [00:00<?, ?it/s]"]},"metadata":{}},{"name":"stdout","output_type":"stream","text":"Epoch 1: binary_evaluator Score = 0.9447816265234429, triplet_evaluator Score = 0.99375, ir_evaluator Score = 0.8752380952380951\n\nEpoch 2: binary_evaluator Score = 0.957717869564054, triplet_evaluator Score = 0.996875, ir_evaluator Score = 0.8839972527472529\n\nEpoch 3: binary_evaluator Score = 0.9678803045385579, triplet_evaluator Score = 0.996875, ir_evaluator Score = 0.8886607142857142\n\nEpoch 4: binary_evaluator Score = 0.9722261108198784, triplet_evaluator Score = 0.996875, ir_evaluator Score = 0.9022023809523809\n\nEpoch 5: binary_evaluator Score = 0.9765535115112803, triplet_evaluator Score = 0.996875, ir_evaluator Score = 0.8967708333333333\n\nEpoch 6: binary_evaluator Score = 0.9792795897575098, triplet_evaluator Score = 0.996875, ir_evaluator Score = 0.8827083333333334\n\nEpoch 7: binary_evaluator Score = 0.9802939077629669, triplet_evaluator Score = 0.996875, ir_evaluator Score = 0.8827083333333334\n\nEpoch 8: binary_evaluator Score = 0.9834489278804479, triplet_evaluator Score = 0.996875, ir_evaluator Score = 0.8810416666666667\n\nEpoch 9: binary_evaluator Score = 0.9828009559247337, triplet_evaluator Score = 0.996875, ir_evaluator Score = 0.8816666666666666\n\nEpoch 10: binary_evaluator Score = 0.9843386953324966, triplet_evaluator Score = 0.996875, ir_evaluator Score = 0.8879166666666667\n\nEpoch 11: binary_evaluator Score = 0.9841004091488997, triplet_evaluator Score = 0.996875, ir_evaluator Score = 0.8883333333333333\n\nEpoch 12: binary_evaluator Score = 0.9841280261179672, triplet_evaluator Score = 0.996875, ir_evaluator Score = 0.8877083333333333\n\nEarly stopping at epoch 10\n\nBest weights restored.\n"}]},{"cell_type":"code","source":"# Load the saved model\nmodel = SentenceTransformer('celik-muhammed/all-mpnet-base-v2-finetuned-dtc-zoomcamp')\nmodel.max_seq_length = 512\nmodel = model.to('cuda')\nmodel","metadata":{"execution":{"iopub.status.busy":"2024-01-26T05:18:39.387657Z","iopub.execute_input":"2024-01-26T05:18:39.388391Z","iopub.status.idle":"2024-01-26T05:18:39.777342Z","shell.execute_reply.started":"2024-01-26T05:18:39.388352Z","shell.execute_reply":"2024-01-26T05:18:39.776323Z"},"trusted":true},"execution_count":31,"outputs":[{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"SentenceTransformer(\n  (0): Transformer({'max_seq_length': 512, 'do_lower_case': False}) with Transformer model: MPNetModel \n  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': True, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': True, 'pooling_mode_mean_sqrt_len_tokens': True})\n  (2): Dense({'in_features': 3072, 'out_features': 768, 'bias': True, 'activation_function': 'torch.nn.modules.activation.Tanh'})\n  (3): Normalize()\n)"},"metadata":{}}]},{"cell_type":"code","source":"binary_acc_evaluator(model)","metadata":{"execution":{"iopub.status.busy":"2024-01-26T05:18:42.060177Z","iopub.execute_input":"2024-01-26T05:18:42.060565Z","iopub.status.idle":"2024-01-26T05:18:43.757358Z","shell.execute_reply.started":"2024-01-26T05:18:42.060532Z","shell.execute_reply":"2024-01-26T05:18:43.756384Z"},"trusted":true},"execution_count":32,"outputs":[{"execution_count":32,"output_type":"execute_result","data":{"text/plain":"0.9843386953324966"},"metadata":{}}]},{"cell_type":"markdown","source":"## Prediction on Training Data\n\nGenerates predictions on the training data and appends them to the dataframe.","metadata":{}},{"cell_type":"code","source":"try:\n    del train_questions_df['predictions']\nexcept:\n    pass\n\ntrain_questions_df_predict = pd.DataFrame()\n\nfor question_id, question, course, year, candidate_answers, answer_id in tqdm(train_questions_df.values):\n    answers_list = candidate_answers.split(',')\n    \n    candidate_answers_list = []\n    for ans_id in answers_list:\n        candidate_answers_list.append(train_answers_df[train_answers_df.answer_id == int(ans_id)]['answer'].values[0])\n\n    # Adding a single new row\n    new_row = {\n        'question_id': question_id, \n        'question': question, \n        'candidate_answers_id': answers_list,\n        'candidate_answers': candidate_answers_list,\n    }\n\n    train_questions_df_predict = train_questions_df_predict._append(new_row, ignore_index=True)\n    \n    \ntrain_predictions = []\nsimilarity_scores_df = []\nfor question, candidate_answers, candidate_answers_id in zip(\n    tqdm(train_questions_df_predict['question']), \n    train_questions_df_predict['candidate_answers'],\n    train_questions_df_predict['candidate_answers_id']\n):\n    q_embeddings = model.encode([question], show_progress_bar=0)\n    a_embeddings = []    \n    for answer in candidate_answers:\n        a_embedding = np.mean(model.encode(process_sentences(answer), show_progress_bar=0), axis=0)\n        a_embeddings.append(a_embedding)    \n    similarity_scores = cosine_similarity(q_embeddings, a_embeddings)\n    \n    train_predictions.append(int(candidate_answers_id[np.argmax(similarity_scores)]))\n    similarity_scores_df.append(similarity_scores)\n\ntrain_questions_df['predictions'] = train_predictions","metadata":{"execution":{"iopub.status.busy":"2024-01-26T05:18:45.971828Z","iopub.execute_input":"2024-01-26T05:18:45.972286Z","iopub.status.idle":"2024-01-26T05:19:42.020991Z","shell.execute_reply.started":"2024-01-26T05:18:45.972249Z","shell.execute_reply":"2024-01-26T05:19:42.020023Z"},"trusted":true},"execution_count":33,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/396 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ba2812317e2f49b5b7f4c0c099beb4f0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/396 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b01fa4e114d041b097d0166aa91bf696"}},"metadata":{}}]},{"cell_type":"markdown","source":"## Evaluation\n\nCalculates and prints the accuracy of the model on the training data.","metadata":{}},{"cell_type":"code","source":"# Accuracy calculation\naccuracy_full  = (train_questions_df['predictions'] == train_questions_df['answer_id']).mean()\nprint(f'Accuracy full : {accuracy_full:.4f}')\n\naccuracy_val   = train_questions_df[train_questions_df['question_id'].isin(val_df['question_id']  )].apply(lambda row: row['predictions']==row['answer_id'], axis=1).mean()\naccuracy_train = train_questions_df[train_questions_df['question_id'].isin(train_df['question_id'])].apply(lambda row: row['predictions']==row['answer_id'], axis=1).mean()\nprint(f'Accuracy val  : {accuracy_val:.4f}')\nprint(f'Accuracy train: {accuracy_train:.4f}')","metadata":{"execution":{"iopub.status.busy":"2024-01-26T05:19:42.023089Z","iopub.execute_input":"2024-01-26T05:19:42.023797Z","iopub.status.idle":"2024-01-26T05:19:42.043100Z","shell.execute_reply.started":"2024-01-26T05:19:42.023738Z","shell.execute_reply":"2024-01-26T05:19:42.041937Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stdout","text":"Accuracy full : 0.9949\nAccuracy val  : 0.9750\nAccuracy train: 1.0000\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Prediction and Submission for Test Data\n\nGenerates predictions for the test data and creates a submission file for the Kaggle competition.","metadata":{}},{"cell_type":"code","source":"try:\n    del test_questions_df['predicted_answer_id']\nexcept:\n    pass\n\ntest_questions_df = test_questions_df.drop_duplicates(subset='question_id')\ntest_questions_df.shape\n\ntest_questions_df_predict = pd.DataFrame()\n\nfor question_id, question, course, year, candidate_answers in tqdm(test_questions_df.values):\n    answers_list = candidate_answers.split(',')\n    candidate_answers_list = []\n\n    for ans_id in answers_list:\n        candidate_answers_list.append(test_answers_df[test_answers_df.answer_id == int(ans_id)]['answer'].values[0])\n\n    # Adding a single new row\n    new_row = {\n        'question_id': question_id, \n        'question': question, \n        'candidate_answers_id': answers_list,\n        'candidate_answers': candidate_answers_list,\n    }\n\n    test_questions_df_predict = test_questions_df_predict._append(new_row, ignore_index=True)\n    \n    \ntest_predictions = []\nsimilarity_scores_df = []\nfor question, candidate_answers, candidate_answers_id in zip(\n    tqdm(test_questions_df_predict['question']), \n    test_questions_df_predict['candidate_answers'],\n    test_questions_df_predict['candidate_answers_id']\n):\n    q_embeddings = model.encode([question], show_progress_bar=0)\n    a_embeddings = []    \n    for answer in candidate_answers:\n        a_embedding = np.mean(model.encode(process_sentences(answer), show_progress_bar=0), axis=0)\n        a_embeddings.append(a_embedding)    \n    similarity_scores = cosine_similarity(q_embeddings, a_embeddings)\n    \n    test_predictions.append(int(candidate_answers_id[np.argmax(similarity_scores)]))\n    similarity_scores_df.append(similarity_scores)\n\ntest_questions_df['predicted_answer_id'] = test_predictions","metadata":{"execution":{"iopub.status.busy":"2024-01-26T05:19:42.044391Z","iopub.execute_input":"2024-01-26T05:19:42.044733Z","iopub.status.idle":"2024-01-26T05:20:44.708795Z","shell.execute_reply.started":"2024-01-26T05:19:42.044700Z","shell.execute_reply":"2024-01-26T05:20:44.707789Z"},"trusted":true},"execution_count":35,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/514 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"88c3684a536e47d983406961f37de7ac"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/514 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9eb8b0e78fbb43dbb11629aa6059a24d"}},"metadata":{}}]},{"cell_type":"code","source":"test_questions_df['predicted_answer_id'].nunique()","metadata":{"execution":{"iopub.status.busy":"2024-01-26T05:20:44.711863Z","iopub.execute_input":"2024-01-26T05:20:44.712600Z","iopub.status.idle":"2024-01-26T05:20:44.721500Z","shell.execute_reply.started":"2024-01-26T05:20:44.712559Z","shell.execute_reply":"2024-01-26T05:20:44.720408Z"},"trusted":true},"execution_count":36,"outputs":[{"execution_count":36,"output_type":"execute_result","data":{"text/plain":"491"},"metadata":{}}]},{"cell_type":"code","source":"test_questions_df[['question_id', 'predicted_answer_id']].to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2024-01-26T05:20:44.722636Z","iopub.execute_input":"2024-01-26T05:20:44.723009Z","iopub.status.idle":"2024-01-26T05:20:44.737978Z","shell.execute_reply.started":"2024-01-26T05:20:44.722980Z","shell.execute_reply":"2024-01-26T05:20:44.737065Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"submission_df = pd.read_csv(\"/kaggle/working/submission.csv\")\nsubmission_df.head()","metadata":{"execution":{"iopub.status.busy":"2024-01-26T05:20:44.739322Z","iopub.execute_input":"2024-01-26T05:20:44.739645Z","iopub.status.idle":"2024-01-26T05:20:44.751443Z","shell.execute_reply.started":"2024-01-26T05:20:44.739617Z","shell.execute_reply":"2024-01-26T05:20:44.750361Z"},"trusted":true},"execution_count":38,"outputs":[{"execution_count":38,"output_type":"execute_result","data":{"text/plain":"   question_id  predicted_answer_id\n0          707               767296\n1       534450               573165\n2       996163               571892\n3       860215               988549\n4       980124               384381","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>question_id</th>\n      <th>predicted_answer_id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>707</td>\n      <td>767296</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>534450</td>\n      <td>573165</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>996163</td>\n      <td>571892</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>860215</td>\n      <td>988549</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>980124</td>\n      <td>384381</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# Save Versions\n# now = datetime.datetime.now().strftime(\"%d-%b-%Y %H-%M-%S\")\n# now = pd.Timestamp.now().strftime(\"%d-%b-%Y %H-%M-%S\")\n# np.save(now, np.array([now]))\n\nfrom IPython.display import FileLink, FileLinks\nlocal_file = FileLink(r'submission.csv', result_html_prefix=\"Click here to download: \")\nlocal_file","metadata":{"execution":{"iopub.status.busy":"2024-01-26T05:20:44.752667Z","iopub.execute_input":"2024-01-26T05:20:44.753030Z","iopub.status.idle":"2024-01-26T05:20:44.760888Z","shell.execute_reply.started":"2024-01-26T05:20:44.753002Z","shell.execute_reply":"2024-01-26T05:20:44.759967Z"},"trusted":true},"execution_count":39,"outputs":[{"execution_count":39,"output_type":"execute_result","data":{"text/plain":"/kaggle/working/submission.csv","text/html":"Click here to download: <a href='submission.csv' target='_blank'>submission.csv</a><br>"},"metadata":{}}]},{"cell_type":"markdown","source":"## Save and Push Model Huggingface Hub\n\n- https://huggingface.co/docs/transformers/v4.15.0/model_sharing#directly-push-your-model-to-the-hub","metadata":{}},{"cell_type":"code","source":"model.save(\".\", model_name=save_to_hub_model_id)","metadata":{"execution":{"iopub.status.busy":"2024-01-26T05:21:14.403652Z","iopub.execute_input":"2024-01-26T05:21:14.404512Z","iopub.status.idle":"2024-01-26T05:21:14.410779Z","shell.execute_reply.started":"2024-01-26T05:21:14.404472Z","shell.execute_reply":"2024-01-26T05:21:14.409654Z"},"trusted":true},"execution_count":41,"outputs":[{"execution_count":41,"output_type":"execute_result","data":{"text/plain":"{'__version__': {'sentence_transformers': '2.2.2',\n  'transformers': '4.36.2',\n  'pytorch': '2.0.0'}}"},"metadata":{}}]},{"cell_type":"code","source":"# import huggingface_hub as hf\n# from huggingface_hub import HfApi\n# import json\n\n# # Ensure you are logged in\n# # hf.login()\n\n# HUGGINGFACE_TOKEN = ''\n# if HUGGINGFACE_TOKEN:\n#     !huggingface-cli login --token $HUGGINGFACE_TOKEN --add-to-git-credential\n    \n#     # Replace 'username/model' with the actual username and model name on the Hugging Face Model Hub\n#     save_to_hub_model_id = f\"{hf.whoami()['name']}/{model_checkpoint.split('/')[-1]}-finetuned-dtc-zoomcamp\"\n    \n#     # Save the model in local as safetensors\n#     model.save(save_to_hub_model_id, model_name=save_to_hub_model_id)\n#     # Save the model in PyTorch format using native PyTorch functions\n#     torch.save(model.state_dict(), save_to_hub_model_id+'/pytorch_model.bin')\n        \n#     # update config file\n#     model.save(save_to_hub_model_id, model_name=save_to_hub_model_id)\n\n#     # Push the tokenizer\n#     tokenizer.push_to_hub(save_to_hub_model_id)\n#     # Push to Hub all model Safetensors, PyTorch format\n#     api = HfApi()\n#     api.upload_folder(\n#         repo_id=save_to_hub_model_id,\n#         folder_path=save_to_hub_model_id,\n#         repo_type=\"model\",\n#     )","metadata":{"execution":{"iopub.execute_input":"2024-01-25T16:46:35.882995Z","iopub.status.busy":"2024-01-25T16:46:35.881925Z","iopub.status.idle":"2024-01-25T16:47:19.095306Z","shell.execute_reply":"2024-01-25T16:47:19.094158Z","shell.execute_reply.started":"2024-01-25T16:46:35.882953Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"## Check Saved Model","metadata":{}},{"cell_type":"code","source":"from sentence_transformers import SentenceTransformer\nsentences = ['This is an example sentence', 'Each sentence is converted']\n\nmodel_checkpoint = 'celik-muhammed/all-mpnet-base-v2-finetuned-dtc-zoomcamp'\nmodel      = SentenceTransformer(model_checkpoint)\nembeddings = model.encode(sentences)\nprint(embeddings.shape)","metadata":{"execution":{"iopub.status.busy":"2024-01-26T05:26:00.729079Z","iopub.execute_input":"2024-01-26T05:26:00.729821Z","iopub.status.idle":"2024-01-26T05:26:01.148714Z","shell.execute_reply.started":"2024-01-26T05:26:00.729787Z","shell.execute_reply":"2024-01-26T05:26:01.147723Z"},"trusted":true},"execution_count":44,"outputs":[{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"351ea0e7ce5b420591b8a503e1e18599"}},"metadata":{}},{"name":"stdout","text":"(2, 768)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Export to TFLite\n\n- https://www.tensorflow.org/lite/guide/ops_select\n- https://huggingface.co/docs/transformers/main/en/tflite#export-to-tflite","metadata":{}},{"cell_type":"code","source":"from transformers import AutoConfig, AutoTokenizer, AutoModel, TFAutoModel\nimport tensorflow as tf; tf.keras.backend.clear_session()\n\n## Load the model (architecture + weights), from safetensors\nmodel_checkpoint = 'celik-muhammed/all-mpnet-base-v2-finetuned-dtc-zoomcamp'\nmodel     = TFAutoModel.from_pretrained(model_checkpoint, from_pt=False)\n## Set the entire model to be non-trainable\nmodel.trainable = False\nmodel.compile()\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2024-01-26T05:26:09.092572Z","iopub.execute_input":"2024-01-26T05:26:09.093425Z","iopub.status.idle":"2024-01-26T05:26:24.155895Z","shell.execute_reply.started":"2024-01-26T05:26:09.093389Z","shell.execute_reply":"2024-01-26T05:26:24.154952Z"},"trusted":true},"execution_count":45,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/613 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7007aa3925874cf7b1101e5e7d879bbe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"acad6d4f56444c498ab55b614a02d9f2"}},"metadata":{}},{"name":"stderr","text":"All PyTorch model weights were used when initializing TFMPNetModel.\n\nAll the weights of TFMPNetModel were initialized from the PyTorch model.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFMPNetModel for predictions without further training.\n","output_type":"stream"},{"name":"stdout","text":"Model: \"tfmp_net_model\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n mpnet (TFMPNetMainLayer)    multiple                  109486464 \n                                                                 \n=================================================================\nTotal params: 109486464 (417.66 MB)\nTrainable params: 0 (0.00 Byte)\nNon-trainable params: 109486464 (417.66 MB)\n_________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":"import tensorflow as tf\n\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\nconverter.target_spec.supported_types = [tf.float16]\nconverter.target_spec.supported_ops = [\n  tf.lite.OpsSet.TFLITE_BUILTINS, # enable TensorFlow Lite ops\n  tf.lite.OpsSet.SELECT_TF_OPS, # enable TensorFlow ops\n]\ntflite_model = converter.convert()\nwith open('mpnet-dtc-zoomcamp_tfmodel.tflite', 'wb') as f_out:\n    f_out.write(tflite_model)","metadata":{"execution":{"iopub.status.busy":"2024-01-26T05:26:41.856428Z","iopub.execute_input":"2024-01-26T05:26:41.856832Z","iopub.status.idle":"2024-01-26T05:27:51.136728Z","shell.execute_reply.started":"2024-01-26T05:26:41.856799Z","shell.execute_reply":"2024-01-26T05:27:51.135872Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"markdown","source":"## Dummy Testing Model and Prepare for Containerization","metadata":{}},{"cell_type":"code","source":"!pip install -U https://github.com/alexeygrigorev/tflite-aws-lambda/raw/main/tflite/tflite_runtime-2.14.0-cp310-cp310-linux_x86_64.whl","metadata":{"execution":{"iopub.status.busy":"2024-01-26T05:29:08.174393Z","iopub.execute_input":"2024-01-26T05:29:08.175505Z","iopub.status.idle":"2024-01-26T05:29:21.825270Z","shell.execute_reply.started":"2024-01-26T05:29:08.175454Z","shell.execute_reply":"2024-01-26T05:29:21.824037Z"},"trusted":true},"execution_count":48,"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Collecting tflite-runtime==2.14.0\n  Downloading https://github.com/alexeygrigorev/tflite-aws-lambda/raw/main/tflite/tflite_runtime-2.14.0-cp310-cp310-linux_x86_64.whl (2.4 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m39.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: numpy>=1.23.2 in /opt/conda/lib/python3.10/site-packages (from tflite-runtime==2.14.0) (1.24.3)\nInstalling collected packages: tflite-runtime\nSuccessfully installed tflite-runtime-2.14.0\n","output_type":"stream"}]},{"cell_type":"code","source":"# !pip install -U https://github.com/alexeygrigorev/tflite-aws-lambda/raw/main/tflite/tflite_runtime-2.14.0-cp310-cp310-linux_x86_64.whl\nimport requests\nimport numpy as np\nimport tflite_runtime.interpreter as tflite\nimport json  # Import the json module\n# Disable TensorFlow warnings, before you import tensorflow\nimport os; os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\nimport gc; gc.collect()\n\n# Replace this random input with your actual question and answers\nquestion    = \"That is a happy person\"\nanswers     = [\"That is a happy dog\", \"That is a very happy person\", \"Today is a sunny day\"]\ndummy_input = np.array([[1, 1, 1]], dtype=np.int32)\n\n# Load the TFLite model in TFLite Interpreter and allocate tensors.\ninterpreter = tflite.Interpreter(model_path='mpnet-dtc-zoomcamp_tfmodel.tflite')\ninterpreter.allocate_tensors()\n\n# Get input and output tensors.\n# input_details    = interpreter.get_input_details()\n# output_details   = interpreter.get_output_details()\n\n# Print the signatures from the converted model\n# {'serving_default': {'inputs': ['attention_mask', 'input_ids'], 'outputs': ['last_hidden_state', 'pooler_output']}}\ninterpreter.get_signature_list()\n\ninfer  = interpreter.get_signature_runner(\"serving_default\")\noutput = infer(input_ids=dummy_input, attention_mask=dummy_input)['last_hidden_state'].squeeze()\noutput = np.mean(output, axis=0)\n\n# Convert the result to a JSON-formatted string\nresult_json = json.dumps({\"shape\": output.shape}, indent=2)\nprint(result_json)","metadata":{"execution":{"iopub.status.busy":"2024-01-26T05:31:06.017462Z","iopub.execute_input":"2024-01-26T05:31:06.017910Z","iopub.status.idle":"2024-01-26T05:31:07.687081Z","shell.execute_reply.started":"2024-01-26T05:31:06.017870Z","shell.execute_reply":"2024-01-26T05:31:07.685885Z"},"trusted":true},"execution_count":49,"outputs":[{"name":"stderr","text":"INFO: Created TensorFlow Lite delegate for select TF ops.\nINFO: TfLiteFlexDelegate delegate: 4 nodes delegated out of 1350 nodes with 1 partitions.\n\nINFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n","output_type":"stream"},{"name":"stdout","text":"{\n  \"shape\": [\n    768\n  ]\n}\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Testing function for Lambda","metadata":{}},{"cell_type":"code","source":"# !pip install -U https://github.com/alexeygrigorev/tflite-aws-lambda/raw/main/tflite/tflite_runtime-2.14.0-cp310-cp310-linux_x86_64.whl\nimport requests\nimport numpy as np\nimport tflite_runtime.interpreter as tflite\nimport json  # Import the json module\n# Disable TensorFlow warnings, before you import tensorflow\nimport os; os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\nimport gc; gc.collect()\nfrom transformers import AutoTokenizer\n# Use a tokenizer appropriate for your model (replace \"bert-base-uncased\" with your model name)\ntokenizer = AutoTokenizer.from_pretrained('celik-muhammed/all-mpnet-base-v2-finetuned-dtc-zoomcamp')\n\n# Replace this random input with your actual question and answers\nquestion = \"That is a happy person\"\nanswers = [\"That is a happy dog\", \"That is a very happy person\", \"Today is a sunny day\"]\n# Tokenize the question and answers\ntokenized_question = tokenizer(question, return_tensors=\"np\", padding=True, truncation=True, max_length=128)\n\n# Load the TFLite model in TFLite Interpreter and allocate tensors.\ninterpreter = tflite.Interpreter(model_path='mpnet-dtc-zoomcamp_tfmodel.tflite')\ninterpreter.allocate_tensors()\n\n# Get input and output tensors.\n# input_details    = interpreter.get_input_details()\n# output_details   = interpreter.get_output_details()\n# {'serving_default': {'inputs': ['attention_mask', 'input_ids'], 'outputs': ['last_hidden_state', 'pooler_output']}}\ninterpreter.get_signature_list()\n\ninfer  = interpreter.get_signature_runner(\"serving_default\")\noutput = infer(\n    input_ids      = np.array(tokenized_question['input_ids'], dtype=np.int32), \n    attention_mask = np.array(tokenized_question['attention_mask'], dtype=np.int32)\n)['last_hidden_state'].squeeze()\n\n# Get the output data\nq_embedding = np.mean(output, axis=0)\n\n# Your q_embedding contains the embeddings. Now, you can use these embeddings\n# to calculate cosine similarity between the question and each answers.\n\n# Replace this with your actual processing of each answer\na_embeddings = []\nfor i, answer in enumerate(answers):\n    # Process each answer with your model and obtain the embedding\n    tokenized_answers  = tokenizer(answer, return_tensors=\"np\", padding=True, truncation=True, max_length=128)\n    output = infer(\n        input_ids      = np.array(tokenized_answers['input_ids'], dtype=np.int32),\n        attention_mask = np.array(tokenized_answers['attention_mask'], dtype=np.int32)\n    )['last_hidden_state'].squeeze()\n    a_embedding = np.mean(output, axis=0)\n    \n    # Calculate cosine similarity between the question and the current answer\n    similarity = np.dot(q_embedding, a_embedding) / (np.linalg.norm(q_embedding) * np.linalg.norm(a_embedding))\n    a_embeddings.append({\"answer\": answer, \"similarity\": f\"{similarity:.3f}\"})\n\n# Convert the result to a JSON-formatted string\nresult_json = json.dumps({\"question\": question, \"results\": a_embeddings}, indent=2)\nprint(result_json)","metadata":{"execution":{"iopub.status.busy":"2024-01-26T05:31:17.413030Z","iopub.execute_input":"2024-01-26T05:31:17.413420Z","iopub.status.idle":"2024-01-26T05:31:19.336790Z","shell.execute_reply.started":"2024-01-26T05:31:17.413389Z","shell.execute_reply":"2024-01-26T05:31:19.335720Z"},"trusted":true},"execution_count":51,"outputs":[{"name":"stdout","text":"{\n  \"question\": \"That is a happy person\",\n  \"results\": [\n    {\n      \"answer\": \"That is a happy dog\",\n      \"similarity\": \"0.760\"\n    },\n    {\n      \"answer\": \"That is a very happy person\",\n      \"similarity\": \"0.971\"\n    },\n    {\n      \"answer\": \"Today is a sunny day\",\n      \"similarity\": \"0.355\"\n    }\n  ]\n}\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Convert .ipynb to .py\n\nBash code for converting ipynb to py\n\n> jupyter nbconvert --to script model.ipynb\n\nlambda function a function must be added as below to the lambda_function.py file:\n\n\n```py\ndef lambda_handler(event, context):\n    url = event['url']\n    result = predict(url)\n    return result\n```","metadata":{}},{"cell_type":"code","source":"%%writefile lambda_function.py\n#!/usr/bin/env python\n# coding: utf-8\n\n# !pip install -U https://github.com/alexeygrigorev/tflite-aws-lambda/raw/main/tflite/tflite_runtime-2.14.0-cp310-cp310-linux_x86_64.whl\nimport requests\nimport numpy as np\nimport tflite_runtime.interpreter as tflite\nimport json  # Import the json module\n# Disable TensorFlow warnings, before you import tensorflow\nimport os; os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\nimport gc; gc.collect()\nfrom transformers import AutoTokenizer\n# Use a tokenizer appropriate for your model (replace \"bert-base-uncased\" with your model name)\ntokenizer = AutoTokenizer.from_pretrained('celik-muhammed/all-mpnet-base-v2-finetuned-dtc-zoomcamp')\n\n# Load the TFLite model in TFLite Interpreter and allocate tensors.\ninterpreter = tflite.Interpreter(model_path='mpnet-dtc-zoomcamp_tfmodel.tflite')\ninterpreter.allocate_tensors()\ninfer  = interpreter.get_signature_runner(\"serving_default\")\n\ndef query(payload):\n    # Replace this random input with your actual question and answers\n    question = payload['question']\n    answers  = payload['answers']\n    \n    # Tokenize the question and answers\n    tokenized_question = tokenizer(question, return_tensors=\"np\", padding=True, truncation=True, max_length=128)\n    output = infer(\n        input_ids      = np.array(tokenized_question['input_ids'], dtype=np.int32), \n        attention_mask = np.array(tokenized_question['attention_mask'], dtype=np.int32)\n    )['last_hidden_state'].squeeze()\n    # Get the output data\n    q_embedding = np.mean(output, axis=0)\n\n    # Replace this with your actual processing of each answer\n    a_embeddings = []\n    for i, answer in enumerate(answers):\n        # Process each answer with your model and obtain the embedding\n        tokenized_answers  = tokenizer(answer, return_tensors=\"np\", padding=True, truncation=True, max_length=128)\n        output = infer(\n            input_ids      = np.array(tokenized_answers['input_ids'], dtype=np.int32),\n            attention_mask = np.array(tokenized_answers['attention_mask'], dtype=np.int32)\n        )['last_hidden_state'].squeeze()\n        a_embedding = np.mean(output, axis=0)\n\n        # Calculate cosine similarity between the question and the current answer\n        similarity = np.dot(q_embedding, a_embedding) / (np.linalg.norm(q_embedding) * np.linalg.norm(a_embedding))\n        a_embeddings.append({\"answer\": answer, \"similarity\": f\"{similarity:.3f}\"})\n\n    # Convert the result to a JSON-formatted string\n    result_json = json.dumps({\"question\": question, \"results\": a_embeddings}, indent=2)\n    return result_json\n\ndef lambda_handler(event, context):\n    payload = event['inputs']\n    result = query(payload)\n    return result","metadata":{"execution":{"iopub.status.busy":"2024-01-26T05:31:21.571639Z","iopub.execute_input":"2024-01-26T05:31:21.572094Z","iopub.status.idle":"2024-01-26T05:31:21.582161Z","shell.execute_reply.started":"2024-01-26T05:31:21.572058Z","shell.execute_reply":"2024-01-26T05:31:21.581140Z"},"trusted":true},"execution_count":52,"outputs":[{"name":"stdout","text":"Writing lambda_function.py\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Testing function for Lambda","metadata":{}},{"cell_type":"code","source":"import lambda_function\n\n# Create the event with the URL of the image\nevent = {\n    \"inputs\": {\n    \"question\": \"That is a happy person\",\n    \"answers\" : [\n        \"That is a happy dog\",\n        \"That is a very happy person\",\n        \"Today is a sunny day\"\n        ]\n    },\n}\n\n# Invoke the Lambda function locally\nresult = lambda_function.lambda_handler(event, None)\nprint(result)","metadata":{"execution":{"iopub.status.busy":"2024-01-26T05:31:23.532777Z","iopub.execute_input":"2024-01-26T05:31:23.533687Z","iopub.status.idle":"2024-01-26T05:31:25.483093Z","shell.execute_reply.started":"2024-01-26T05:31:23.533648Z","shell.execute_reply":"2024-01-26T05:31:25.482080Z"},"trusted":true},"execution_count":53,"outputs":[{"name":"stdout","text":"{\n  \"question\": \"That is a happy person\",\n  \"results\": [\n    {\n      \"answer\": \"That is a happy dog\",\n      \"similarity\": \"0.760\"\n    },\n    {\n      \"answer\": \"That is a very happy person\",\n      \"similarity\": \"0.971\"\n    },\n    {\n      \"answer\": \"Today is a sunny day\",\n      \"similarity\": \"0.355\"\n    }\n  ]\n}\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Pipenv for dependency management:","metadata":{}},{"cell_type":"code","source":"%%bash\nget_versions() {\n  for lib in \"$@\"; do\n    info=$(pip show $lib 2>/dev/null)\n    if [ $? -eq 0 ]; then\n      version=$(echo \"$info\" | awk -F ': ' '$1==\"Version\" {print $2}')\n      echo \"$lib==$version\"\n    fi\n  done\n}\n# Example usage\nget_versions  \"transformers\" \"tflite_runtime\" \"numpy\" > requirements.txt\ncat requirements.txt","metadata":{"execution":{"iopub.status.busy":"2024-01-26T05:33:01.570023Z","iopub.execute_input":"2024-01-26T05:33:01.570443Z","iopub.status.idle":"2024-01-26T05:33:35.535919Z","shell.execute_reply.started":"2024-01-26T05:33:01.570409Z","shell.execute_reply":"2024-01-26T05:33:35.534807Z"},"trusted":true},"execution_count":56,"outputs":[{"name":"stdout","text":"transformers==4.36.2\ntflite_runtime==2.14.0\nnumpy==1.24.3\n","output_type":"stream"}]},{"cell_type":"code","source":"# Generate Pipfile file from requirements.txt\n!pipenv --python 3.10\n# Create a virtual environment and install packages\n!pipenv install\n# Update the Pipfile.lock\n!pipenv lock\n# Clean up the temporary virtual environment\n!pipenv --rm\n# Check\n# !ls ~/.local/share/virtualenvs/\n!cat Pipfile","metadata":{"execution":{"iopub.status.busy":"2024-01-26T05:33:38.869228Z","iopub.execute_input":"2024-01-26T05:33:38.869628Z","iopub.status.idle":"2024-01-26T05:34:20.431178Z","shell.execute_reply.started":"2024-01-26T05:33:38.869594Z","shell.execute_reply":"2024-01-26T05:34:20.429957Z"},"trusted":true},"execution_count":57,"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1mCreating a virtualenv for this project...\u001b[0m\nPipfile: \u001b[33m\u001b[1m/kaggle/working/Pipfile\u001b[0m\n\u001b[1mUsing\u001b[0m \u001b[33m\u001b[1m/opt/conda/bin/python3.1\u001b[0m \u001b[32m(3.10.12)\u001b[0m \u001b[1mto create virtualenv...\u001b[0m\n\u001b[2K\u001b[32m⠦\u001b[0m Creating virtual environment.....\u001b[36mcreated virtual environment CPython3.10.12.final.0-64 in 1620ms\n  creator CPython3Posix(dest=/root/.local/share/virtualenvs/working-MLRu3Pvq, clear=False, no_vcs_ignore=False, global=False)\n  seeder FromAppData(download=False, pip=bundle, setuptools=bundle, wheel=bundle, via=copy, app_data_dir=/root/.local/share/virtualenv)\n    added seed packages: pip==23.3.1, setuptools==69.0.2, wheel==0.42.0\n  activators BashActivator,CShellActivator,FishActivator,NushellActivator,PowerShellActivator,PythonActivator\n\u001b[0m\n✔ Successfully created virtual environment!\n\u001b[2K\u001b[32m⠧\u001b[0m Creating virtual environment...\n\u001b[1A\u001b[2K\u001b[32mVirtualenv location: /root/.local/share/virtualenvs/working-MLRu3Pvq\u001b[0m\n\u001b[1mrequirements.txt\u001b[0m found in \u001b[1;33m/kaggle/\u001b[0m\u001b[1;33mworking\u001b[0m instead of \u001b[1mPipfile\u001b[0m! Converting\u001b[33m...\u001b[0m\n\u001b[2K✔ Success! Importing requirements.....\n\u001b[2K\u001b[32m⠸\u001b[0m Importing requirements...\n\u001b[1A\u001b[2K\u001b[1;31mWarning\u001b[0m: Your \u001b[1mPipfile\u001b[0m now contains pinned versions, if your \u001b[1mrequirements.txt\u001b[0m \ndid. \nWe recommend updating your \u001b[1mPipfile\u001b[0m to specify the \u001b[1;32m\"*\"\u001b[0m version, instead.\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1mPipfile.lock not found, creating\u001b[0m\u001b[1;33m...\u001b[0m\nLocking\u001b[0m \u001b[33m[packages]\u001b[0m dependencies...\u001b[0m\n\u001b[?25lBuilding requirements\u001b[33m...\u001b[0m\n\u001b[2KResolving dependencies\u001b[33m...\u001b[0m\n\u001b[2K✔ Success! Locking...\n\u001b[2K\u001b[32m⠹\u001b[0m Locking...\n\u001b[1A\u001b[2KLocking\u001b[0m \u001b[33m[dev-packages]\u001b[0m dependencies...\u001b[0m\n\u001b[1mUpdated Pipfile.lock (02cd654209c8f6373a9812d72a6e911b97765ff86013ea787cebbef2b80b7454)!\u001b[0m\n\u001b[1mInstalling dependencies from Pipfile.lock \u001b[0m\u001b[1m(\u001b[0m\u001b[1m0b7454\u001b[0m\u001b[1m)\u001b[0m\u001b[1;33m...\u001b[0m\nTo activate this project's virtualenv, run \u001b[33mpipenv shell\u001b[0m.\nAlternatively, run a command inside the virtualenv with \u001b[33mpipenv run\u001b[0m.\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Locking\u001b[0m \u001b[33m[packages]\u001b[0m dependencies...\u001b[0m\n\u001b[?25lBuilding requirements\u001b[33m...\u001b[0m\n\u001b[2KResolving dependencies\u001b[33m...\u001b[0m\n\u001b[2K✔ Success! Locking...\n\u001b[2K\u001b[32m⠧\u001b[0m Locking...\n\u001b[1A\u001b[2KLocking\u001b[0m \u001b[33m[dev-packages]\u001b[0m dependencies...\u001b[0m\n\u001b[1mUpdated Pipfile.lock (02cd654209c8f6373a9812d72a6e911b97765ff86013ea787cebbef2b80b7454)!\u001b[0m\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1mRemoving virtualenv\u001b[0m \u001b[1m(\u001b[0m\u001b[32m/root/.local/share/virtualenvs/\u001b[0m\u001b[32mworking-MLRu3Pvq\u001b[0m\u001b[1;32m)\u001b[0m\u001b[32m...\u001b[0m\n\u001b[2K\u001b[32m⠹\u001b[0m Running.....\n\u001b[1A\u001b[2K","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"[[source]]\nurl = \"https://pypi.org/simple\"\nverify_ssl = true\nname = \"pypi\"\n\n[packages]\ntransformers = \"==4.36.2\"\ntflite-runtime = \"==2.14.0\"\nnumpy = \"==1.24.3\"\n\n[dev-packages]\n\n[requires]\npython_version = \"3.10\"\npython_full_version = \"3.10.12\"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Preparing Docker Image\n\nBuild docker image using the recommended public image for Lambda once Dockerfile has been created below:\n\n- https://repost.aws/knowledge-center/lambda-container-images\n\n> docker build -t dtc-zoomcamp-q-a-challenge .\n\nTo test first run image that was built:\n\n> docker run --gpus all -it --rm -p 9000:8080 dtc-zoomcamp-q-a-challenge:latest","metadata":{}},{"cell_type":"code","source":"%%writefile Dockerfile\n# public image for Lambda\nFROM public.ecr.aws/lambda/python:3.10\n\n# Copy the Pipfile and Pipfile.lock into the container\n# COPY [\"requirements.txt\", \"./\"]\n# RUN pip install -r requirements.txt\n\n# recompiled with the lambda image\nRUN pip install --upgrade pip\nRUN pip install -U https://github.com/alexeygrigorev/tflite-aws-lambda/raw/main/tflite/tflite_runtime-2.14.0-cp310-cp310-linux_x86_64.whl\nRUN pip install -U transformers\nRUN pip install -U numpy\n\n# Copy function code and model into the container\nCOPY [\"lambda_function.py\", \"mpnet-dtc-zoomcamp_tfmodel.tflite\", \"./\"]\n\n# Set the CMD to your handler \nCMD [ \"lambda_function.lambda_handler\" ]","metadata":{"execution":{"iopub.status.busy":"2024-01-26T05:34:42.298295Z","iopub.execute_input":"2024-01-26T05:34:42.299322Z","iopub.status.idle":"2024-01-26T05:34:42.307215Z","shell.execute_reply.started":"2024-01-26T05:34:42.299276Z","shell.execute_reply":"2024-01-26T05:34:42.306119Z"},"trusted":true},"execution_count":58,"outputs":[{"name":"stdout","text":"Writing Dockerfile\n","output_type":"stream"}]},{"cell_type":"code","source":"# run to local\n# docker build -t dtc-zoomcamp-q-a-challenge .\n# docker run -it --rm -p 9000:8080 dtc-zoomcamp-q-a-challenge:latest","metadata":{"execution":{"iopub.status.busy":"2024-01-26T05:34:43.518476Z","iopub.execute_input":"2024-01-26T05:34:43.519282Z","iopub.status.idle":"2024-01-26T05:34:43.523457Z","shell.execute_reply.started":"2024-01-26T05:34:43.519244Z","shell.execute_reply":"2024-01-26T05:34:43.522412Z"},"trusted":true},"execution_count":59,"outputs":[]},{"cell_type":"markdown","source":"## test.py created per AWS documentation for testing\n\nRun the file:\n\n> python client_to_docker_test.py\n\nThis is the output I recieved which clearly shows that the image was predicted as a \"dent\" which is correct:\n\n```html\n{\n  \"question\": \"That is a happy person\",\n  \"results\": [\n    {\n      \"answer\": \"That is a happy dog\",\n      \"similarity\": \"0.760\"\n    },\n    {\n      \"answer\": \"That is a very happy person\",\n      \"similarity\": \"0.971\"\n    },\n    {\n      \"answer\": \"Today is a sunny day\",\n      \"similarity\": \"0.355\"\n    }\n  ]\n}\n```","metadata":{}},{"cell_type":"code","source":"%%writefile client_to_docker_test.py\nimport requests\n\n# curl -XPOST \"http://localhost:9000/2015-03-31/functions/function/invocations\" -d '{}'\nurl = 'http://localhost:9000/2015-03-31/functions/function/invocations'\n\n# Create the event with the URL of the image\nevent = {\n    \"inputs\": {\n    \"question\": \"That is a happy person\",\n    \"answers\" : [\n        \"That is a happy dog\",\n        \"That is a very happy person\",\n        \"Today is a sunny day\"\n        ]\n    },\n}\n\n# Send POST request using requests module\nresponse = requests.post(url, json=event)\n# Print the response\nprint(response.text)","metadata":{"execution":{"iopub.status.busy":"2024-01-26T05:34:46.079928Z","iopub.execute_input":"2024-01-26T05:34:46.080354Z","iopub.status.idle":"2024-01-26T05:34:46.087129Z","shell.execute_reply.started":"2024-01-26T05:34:46.080319Z","shell.execute_reply":"2024-01-26T05:34:46.086117Z"},"trusted":true},"execution_count":60,"outputs":[{"name":"stdout","text":"Writing client_to_docker_test.py\n","output_type":"stream"}]},{"cell_type":"code","source":"# to local, sometimes work or not work \n!python client_to_docker_test.py","metadata":{},"execution_count":42,"outputs":[{"name":"stdout","output_type":"stream","text":"{\"errorMessage\": \"Select TensorFlow op(s), included in the given model, is(are) not supported by this interpreter. Make sure you apply/link the Flex delegate before inference. For the Android, it can be resolved by adding \\\"org.tensorflow:tensorflow-lite-select-tf-ops\\\" dependency. See instructions: https://www.tensorflow.org/lite/guide/ops_selectNode number 241 (FlexRealDiv) failed to prepare.\", \"errorType\": \"RuntimeError\", \"requestId\": \"215971ae-cde1-485b-b15c-39ba19645a33\", \"stackTrace\": [\"  File \\\"/var/task/lambda_function.py\\\", line 56, in lambda_handler\\n    result = query(payload)\\n\", \"  File \\\"/var/task/lambda_function.py\\\", line 28, in query\\n    output = infer(\\n\", \"  File \\\"/var/lang/lib/python3.10/site-packages/tflite_runtime/interpreter.py\\\", line 249, in __call__\\n    self._interpreter_wrapper.Invoke(self._subgraph_index)\\n\"]}\n"}]},{"cell_type":"markdown","source":"## Docker Hub","metadata":{}},{"cell_type":"code","source":"# Tag the Existing Image, username/car-insurance-model:new-tag\n# docker tag dtc-zoomcamp-q-a-challenge:latest developerhost/dtc-zoomcamp-q-a-challenge:latest\n\n# Push the newly tagged image to Docker Hub:\n# docker push developerhost/dtc-zoomcamp-q-a-challenge:latest\n\n# you can pull the image:\n# docker pull developerhost/dtc-zoomcamp-q-a-challenge:latest","metadata":{"execution":{"iopub.status.busy":"2024-01-26T05:36:22.082164Z","iopub.execute_input":"2024-01-26T05:36:22.082562Z","iopub.status.idle":"2024-01-26T05:36:22.087027Z","shell.execute_reply.started":"2024-01-26T05:36:22.082528Z","shell.execute_reply":"2024-01-26T05:36:22.085998Z"},"trusted":true},"execution_count":62,"outputs":[]},{"cell_type":"markdown","source":"## End of the Project","metadata":{}}]}